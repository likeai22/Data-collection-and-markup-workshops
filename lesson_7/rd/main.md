## Отчет о выполненной работе по автоматизации сбора данных с Reddit

#### Цель проекта и описание проблемы
Основной целью проекта была автоматизация процесса сбора информации о новых постах на сайте Reddit, включая данные об авторах, названиях сабреддитов, заголовках постов, рейтингах и количестве комментариев. Полученные данные предназначались для последующего анализа в датасетах, что позволило бы проводить исследования и формировать стратегии на основе актуальных данных из сообщества Reddit.

#### Использованные технологии и подходы
1. **Selenium для навигации и автоматизации**: Для автоматизации переходов по сабреддитам и прокрутки страниц с постами использовалась библиотека Selenium. Это позволило имитировать действия пользователя, необходимые для загрузки контента, который динамически подгружается при скроллинге.
2. **BeautifulSoup для парсинга HTML**: Для идентификации и извлечения элементов HTML, содержащих информацию о постах (такую как названия, авторы, рейтинги), использовалась библиотека BeautifulSoup.
3. **Сохранение данных в JSON**: Извлеченные данные сохранялись в структурированном формате JSON, что упрощало их дальнейшую обработку и анализ.
4. **Обработка ошибок и исключений**: В коде была реализована обработка возможных ошибок, таких как отсутствие необходимых элементов на странице или изменение структуры HTML, что повысило надежность и устойчивость скрипта к изменениям на сайте.

#### Основные трудности и их решения
1. **Динамическая загрузка контента**: Страницы Reddit динамически загружают контент при прокрутке, что требовало использования Selenium для автоматизации этого процесса. Решением стала реализация функции, которая имитирует прокрутку до конца страницы, обеспечивая загрузку всего контента.
2. **Потеря данных при скроллинге**: При сохранении проскролленой страницы в файл возникала потеря информации из первых сегментов прокрутки. Для решения этой проблемы было реализовано промежуточное сохранение результатов в отдельные HTML-файлы на каждом этапе прокрутки.
3. **Ограничения запросов к сайту**: Для предотвращения блокировки со стороны Reddit из-за частоты запросов, была введена задержка между запросами и сохранение промежуточных результатов в локальные файлы.

#### Результаты и выводы
Проект успешно реализовал автоматизацию сбора данных с Reddit, позволив собирать обширную информацию о постах в сабреддитах. Полученные данные в формате JSON включали такие поля как "Автор", "Название", "Рейтинг", "Комментарии", что обеспечило богатый источник информации для анализа. Реализация обработки ошибок и исключений повысила надежность скрипта, делая его устойчивым к изменениям в структуре сайта. Этот проект не только упростил процесс сбора данных, но и открыл возможности для дальнейших исследований и аналитики на основе сообщества Reddit.

Пример извлеченных данных в формате JSON может включать следующие поля: "Автор", "Название", "Рейтинг", "Комментарии".
Пример строки:
{
"author": "fanaval",
"title": "[D] GPU and CPU demand for inference in advanced multimodal models",
"rating": "0",
"comments": "7"
},
{
"author": "oz_zey",
"title": "CoRL 2024 reviews [Discussion]",
"rating": "5",
"comments": "8"
}